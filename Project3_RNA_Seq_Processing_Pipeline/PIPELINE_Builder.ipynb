{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are you interested in learning how to handle multiple RNA-seq dataset ??\n",
    "\n",
    "   ### Here is a easy way to learn to build custom pipeline for rna-seq data processing\n",
    "\n",
    "   # Try it!\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "     Here, the goal is to help the beginners to start from scratch and master in designing custom pipeline for\n",
    "     Next-gen sequencing data processing..\n",
    "   \n",
    "##### NOTE: This coding exercise is for the beginners who want to learn the basics of pipeline designing for handing multiple RNA-seq datasets in a single batch. Ofcourse there are other highly advanced pipelines available such as Snakemake (https://snakemake.readthedocs.io/en/stable/ ) and Galaxy (https://usegalaxy.org/) which have a broad application, not just limited to RNA-seq."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use ?\n",
    "\n",
    "### Requirements\n",
    "        Make sure the module(s) are installed in your unix environment\n",
    "        Also, the python packages are compatible with your python version\n",
    "        It is a good idea to do thorough quality check of your raw RNA seq files\n",
    "        using tools such as FASTQC and if required preprocess the reads using read\n",
    "        read editor tools such as Trimmomatic,FASTx toolkits etc.\n",
    "        \n",
    "#### The code is written in python. All you need to do is to run the python script in bash as shown below\n",
    "\n",
    "        Define the parameters as per your choice\n",
    "        Use the module(s) required for data processing\n",
    "        Define the directory of raw fastq file(s)\n",
    "        Define the output directory    \n",
    "        \n",
    "$python Pipeline_Fastq-to-Quant.py -h"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "usage: Pipeline_Fastq-to-Quant.py [-h] [-file [fq [fq ...]]]\n",
    "                                  [-modules [module [module ...]]] [-n N]\n",
    "                                  [-p P] [-w W] [-index INDEX] [-gtf GTF]\n",
    "                                  [-odir ODIR]\n",
    "\n",
    "Custom pipeline to generate script(s) for RNA-seq data processing and quant\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -file [fq [fq ...]]   input fast(q|a) file(s) option:path/*.fastq\n",
    "                        (default: None)\n",
    "  -modules [module [module ...]]\n",
    "                        module(s) required for data processing (default: None)\n",
    "  -n N                  number of nodes (default: 1)\n",
    "  -p P                  number of processors (default: 16)\n",
    "  -w W                  estimated wall time for process (default: 04:00:00)\n",
    "  -index INDEX          <path>/reference index file(s) \n",
    "                        (default:Reference/mouse/index/mm10*)\n",
    "  -gtf GTF              <path>/reference annotation file \n",
    "                        (default:Reference/mouse/mm10.gtf)\n",
    "  -odir ODIR            output directory path (default: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First lets talk a bit more about this pipeline..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work flow of the current pipeline is as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Basically the program is designed to generate script(s) as per the number of fastq files in a given directory.\n",
    "\n",
    "User can customise the JobTemplate to use this script in any cluster depending on the requirements and modules available\n",
    "\n",
    "The work flow includes following 3 mandatory steps:\n",
    "\n",
    "|--Alignment-|-----Post-processing-----||-------------Quantifier-------------| \n",
    "   \n",
    "FASTQ ---> SAM ---> BAM ---> SORTED.BAM ---> Transcript_exp.tab + Gene_exp.tab\n",
    "    Hisat          Samtools              Stringtie\n",
    "\n",
    "Please read the instructions in the manual provided in original webpage of the individual tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait... are you trying for a short cut ?\n",
    "\n",
    "üëÄ\n",
    "\n",
    "üëÄ\n",
    "\n",
    "üëÄ\n",
    "\n",
    "üëÄ\n",
    "\n",
    "üëÄ\n",
    "\n",
    "üëÄ\n",
    "\n",
    "üëÄ\n",
    "\n",
    "üëÄ\n",
    "\n",
    "üëÄ\n",
    "\n",
    "\n",
    "ü§∑‚Äç\n",
    "\n",
    "## Why dont you open the python script ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for python\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "'''\n",
    "You can always customize this code to run your job(s) in High Performance Computing system using PBS or\n",
    "in your local machine. \n",
    "\n",
    "Technically this code is flexible with any cluster. \n",
    "Just change the JobTemplate!\n",
    "'''\n",
    "\n",
    "JobTemplate = \"\"\"\n",
    "#!/bin/bash\n",
    "#PBS -l nodes=%d:ppn=%d \n",
    "#PBS -l walltime=%s \n",
    "#PBS -l gres=ccm\n",
    "#PBS -N %s\"\"\"\n",
    "\n",
    "#ARGPARSE is a beautiful and efficient python package that help you to customise the parameters as per your choice \n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "                    description='Custom pipeline to generate script(s) for RNA-seq data processing and quant',\n",
    "                    formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "                                    )\n",
    "    parser.add_argument('-file', metavar='fq', type = str, nargs='*', help='input fast(q|a) file(s), option:path/*.fastq')\n",
    "    parser.add_argument('-modules', metavar='module', nargs='*', help='module(s) required for data processing')\n",
    "    parser.add_argument('-n', type = int, default = 1, help='number of nodes')\n",
    "    parser.add_argument('-p', type = int, default = 16, help='number of processors')\n",
    "    parser.add_argument('-w', type = str, default = '04:00:00', help='estimated wall time for process')\n",
    "    parser.add_argument('-index', type = str, default = 'Reference/mouse/index/mm10*', help='<path>/reference index file(s)')\n",
    "    parser.add_argument('-gtf', type = str, default = 'Reference/mouse/mm10.gtf', help='<path>/reference annotation file')\n",
    "    parser.add_argument('-odir', type = str, default = '', help='output directory path')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    '''\n",
    "    You know, what is happening here. \n",
    "    We are just defining the parameters based on our arguments\n",
    "    Ofcourse this is not the end, but it will give you an idea on how\n",
    "    to add parameters for your pipeline/ tool.\n",
    "    \n",
    "    \n",
    "    One step smarter now... \n",
    "    \n",
    "    Lets move on..\n",
    "    \n",
    "    Just follow the 3 steps\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    for x in args.file:\n",
    "        x_name = x.split('/')[-1].split('.')[0]\n",
    "        x_dir = os.path.dirname(x)\n",
    "        with open(x + '.sh', 'w') as f:\n",
    "            f.write(JobTemplate %(args.n,args.p,args.w,x_name)+'\\n\\n')       #Did you notice I am calling JobTemplate here!\n",
    "                                                                             #os.path.dirname(x) for calling fastq directory \n",
    "            for m in args.modules:\n",
    "                f.write('module load '+m+'\\n')\n",
    "            f.write('\\ncd '+x_dir+'\\n\\n')\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            STEP 1: ALIGNER\n",
    "            User can add multiple aligner here and make use of it by tweaking in the below code as per the user defined\n",
    "            paramenters in tool guidelines:\n",
    "            \n",
    "            if 'aligner' in str(args.modules):\n",
    "                f.write('ccmrun aligner -p '+str(args.p)+ ' -q -x '+str(args.index)+ ' -u '+x+ ' -S ' + args.odir+x_name+\n",
    "                '.sam'+'\\n')  \n",
    "            ## Customise parameters as per manual instructions\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            if 'hisat' in str(args.modules):\n",
    "                f.write('ccmrun hisat -p '+str(args.p)+ ' -q -x '+str(args.index)+ ' -u '+x+ ' -S ' + args.odir+x_name+\n",
    "                        '.sam'+'\\n')\n",
    "            \n",
    "            '''\n",
    "            STEP 2: POST-PROCESSING\n",
    "            SAMTools is used for post processing of aligned reads. This tool can be customised as per user choice.\n",
    "            In case, the aligner is providing the processed reads, samtool module can be ignored. \n",
    "            Example - TopHat!            \n",
    "            '''      \n",
    "                       \n",
    "            if 'samtools' in str(args.modules):\n",
    "                f.write('ccmrun samtools view -bS '+args.odir+x_name+'.sam > '+ args.odir+x_name+'.bam'+'\\n')\n",
    "                f.write('ccmrun samtools sort '+args.odir+x_name+'.bam '+ args.odir+x_name+'.sorted'+'\\n')\n",
    "                f.write('ccmrun samtools index '+args.odir+x_name+'.sorted.bam'+'\\n')\n",
    "            \n",
    "                        \n",
    "            '''\n",
    "            STEP 3: QUANTIFIER\n",
    "            User can add multiple quantification tool and make use of it by tweaking in the below code as per the user\n",
    "            defined paramenters in tool guidelines:\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            if 'stringtie' in str(args.modules):\n",
    "                f.write('ccmrun stringtie '+args.odir+x_name+'.sorted.bam -p '+str(args.p)+ ' -G '+str(args.gtf)+\n",
    "                        ' -e -o '+args.odir+x_name+'.gtf'+' -A '+args.odir+x_name+'.tab'+'\\n')\n",
    "\n",
    "    return\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take a test run in bash (assuming all the essential requirements are fulfilled)\n",
    "\n",
    "     RUN THIS >>>\n",
    "\n",
    "     $python Pipeline_Fastq-to-Quant.py -file FASTQ/*.py -modules ccm samtools/1.9 stringtie/1.2.3 hisat/3.2.1 -w 08:00:00             -odir FASTQ/DATA/ -n 4 -t 1\n",
    "     \n",
    "#### Did you notice, what is happening here?\n",
    "\n",
    "     We are calling python script with custom parameters such as -w (walltime), -n (number of nodes), \n",
    "     -t (number of processors) , -odir (giving the path for output) and required modules etc to generate\n",
    "     scripts for all the fastq files.\n",
    "     \n",
    "     \n",
    "#### Here is the *output* *script* you will be getting as a Filename.sh:\n",
    "     \n",
    "    #!/bin/bash\n",
    "    #PBS -l nodes=4:ppn=12 \n",
    "    #PBS -l walltime=08:00:00 \n",
    "    #PBS -l gres=ccm\n",
    "    #PBS -N fq1\n",
    "\n",
    "    module load ccm\n",
    "    module load samtools/1.9\n",
    "    module load stringtie/1.2.3\n",
    "    module load hisat/3.2.1\n",
    "\n",
    "    cd FASTQ\n",
    "\n",
    "    ccmrun hisat -p 12 -q -x reference/mouse/index/mm10* -u FASTQ/fq1.py -S FASTQ/DATA/fq1.sam\n",
    "    ccmrun samtools view -bS FASTQ/DATA/fq1.sam > FASTQ/DATA/fq1.bam\n",
    "    ccmrun samtools sort FASTQ/DATA/fq1.bam FASTQ/DATA/fq1.sorted\n",
    "    ccmrun samtools index FASTQ/DATA/fq1.sorted.bam\n",
    "    ccmrun stringtie FASTQ/DATA/fq1.sorted.bam -p 12 -G reference/mouse/mm10.gtf -e -o FASTQ/DATA/fq1.gtf -A FASTQ/DATA/fq1.tab\n",
    "    \n",
    "    \n",
    "    \n",
    "### Can you please look back at the code to revive all the 3 STEPS in the output   \n",
    "     \n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What next...\n",
    "\n",
    "Once you run the python program with customized parameters, you will get the script for each Fastq (Raw RNA seq) file.\n",
    "\n",
    "In Portable Batch System (PBS), you have to run the job as follows:\n",
    "\n",
    "            $qsub Filename.sh\n",
    "\n",
    "However, if you are running multiple scripts, you can que the jobs in the cluster by using following command while in the directory where codes are stored.\n",
    "\n",
    "            $for f in $(find -name '*.sh'); do qsub $f; done\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please share your feedback as comments. If you find it challenging, lets connect we will resolve together\n",
    "\n",
    "\n",
    "### Please follow our page and hashtag to learn about some interesting assignments in coming days..\n",
    "\n",
    "# We hope you enjoyed this tutorial :)  \n",
    "\n",
    "## Happy coding..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
